{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1 0.105 0.11 0.115 0.12 0.125 0.13 0.135 0.14 0.145 0.15 0.155 0.16 0.165 0.17 0.175 0.18 0.185 0.19 0.195 0.2 0.205 0.21 0.215 0.22 0.225 0.23 0.235 0.24 0.245 0.25 0.255 0.26 0.265 0.27 0.275 0.28 0.285 0.29 0.295 0.3 0.305 0.31 0.315 0.32 0.325 0.33 0.335 0.34 0.345 0.35 0.355 0.36 0.365 0.37 0.375 0.38 0.385 0.39 0.395 0.4 0.405 0.41 0.415 0.42 0.425 0.43 0.435 0.44 0.445 0.45 0.455 0.46 0.465 0.47 0.475 0.48 0.485 0.49 0.495 0.5 0.505 0.51 0.515 0.52 0.525 0.53 0.535 0.54 0.545 0.55 0.555 0.56 0.565 0.57 0.575 0.58 0.585 0.59 0.595 0.6 0.605 0.61 0.615 0.62 0.625 0.63 0.635 0.64 0.645 0.65 0.655 0.66 0.665 0.67 0.675 0.68 0.685 0.69 0.695 0.7 0.705 0.71 0.715 0.72 0.725 0.73 0.735 0.74 0.745 0.75 0.755 0.76 0.765 0.77 0.775 0.78 0.785 0.79 0.795 0.8 0.805 0.81 0.815 0.82 0.825 0.83 0.835 0.84 0.845 0.85 0.855 0.86 0.865 0.87 0.875 0.88 0.885 0.89 0.895 0.9 0.905 0.91 0.915 0.92 0.925 0.93 0.935 0.94 0.945 0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1.0 "
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "count = 0\n",
    "with open('CIKM2017_train/train.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print (float(count) / 10000,end=' '),\n",
    "        linenum,label,datas = line.strip().split(',')\n",
    "        label = float(label)\n",
    "        datas = np.asarray(datas.split(' '),dtype=np.int8)\n",
    "        train_x.append(datas)\n",
    "        train_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x,train_y,test_x,test_y = train_x[:9000],train_y[:9000],train_x[9000:],train_y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9000, 612060), 9000, (1000, 612060), 1000)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,len(train_y),test_x.shape,len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x,dtype=np.int8)\n",
    "test_x = np.asarray(test_x,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((train_x,train_y),open('data.pkl','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6121560000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_x[0]) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-61-d9f6b05980f4>:14: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "itertion 0 train loss: 6.82919 test loss: 17.9016\n",
      "itertion 10 train loss: 29.4725 test loss: 17.6738\n",
      "itertion 20 train loss: 20.2323 test loss: 17.4329\n",
      "itertion 30 train loss: 26.3909 test loss: 17.2561\n",
      "itertion 40 train loss: 17.5952 test loss: 17.1334\n",
      "itertion 50 train loss: 13.4382 test loss: 16.9883\n",
      "itertion 60 train loss: 23.0868 test loss: 16.8077\n",
      "itertion 70 train loss: 29.7851 test loss: 16.6406\n",
      "itertion 80 train loss: 24.7055 test loss: 16.5373\n",
      "itertion 90 train loss: 5.89994 test loss: 16.4029\n",
      "itertion 100 train loss: 28.3697 test loss: 16.2601\n",
      "itertion 110 train loss: 18.989 test loss: 16.0822\n",
      "itertion 120 train loss: 24.8171 test loss: 15.9626\n",
      "itertion 130 train loss: 16.8546 test loss: 15.8844\n",
      "itertion 140 train loss: 12.6082 test loss: 15.7857\n",
      "itertion 150 train loss: 21.327 test loss: 15.6634\n",
      "itertion 160 train loss: 28.5929 test loss: 15.5514\n",
      "itertion 170 train loss: 23.9622 test loss: 15.4847\n",
      "itertion 180 train loss: 5.61814 test loss: 15.3918\n",
      "itertion 190 train loss: 27.4613 test loss: 15.3106\n",
      "itertion 200 train loss: 18.2191 test loss: 15.187\n",
      "itertion 210 train loss: 23.6964 test loss: 15.1118\n",
      "itertion 220 train loss: 16.2858 test loss: 15.0663\n",
      "itertion 230 train loss: 12.192 test loss: 15.0039\n",
      "itertion 240 train loss: 19.917 test loss: 14.927\n",
      "itertion 250 train loss: 27.6142 test loss: 14.8576\n",
      "itertion 260 train loss: 23.3586 test loss: 14.8181\n",
      "itertion 270 train loss: 5.80025 test loss: 14.759\n",
      "itertion 280 train loss: 26.7118 test loss: 14.7189\n",
      "itertion 290 train loss: 17.7963 test loss: 14.6405\n",
      "itertion 300 train loss: 22.9216 test loss: 14.5978\n",
      "itertion 310 train loss: 15.8505 test loss: 14.5746\n",
      "itertion 320 train loss: 12.0669 test loss: 14.5391\n",
      "itertion 330 train loss: 18.7933 test loss: 14.4958\n",
      "itertion 340 train loss: 26.8098 test loss: 14.4578\n",
      "itertion 350 train loss: 22.8674 test loss: 14.4375\n",
      "itertion 360 train loss: 6.23232 test loss: 14.4048\n",
      "itertion 370 train loss: 26.0921 test loss: 14.3892\n",
      "itertion 380 train loss: 17.6143 test loss: 14.3468\n",
      "itertion 390 train loss: 22.4033 test loss: 14.3267\n",
      "itertion 400 train loss: 15.5178 test loss: 14.3175\n",
      "itertion 410 train loss: 12.1293 test loss: 14.301\n",
      "itertion 420 train loss: 17.9013 test loss: 14.2812\n",
      "itertion 430 train loss: 26.1476 test loss: 14.2654\n",
      "itertion 440 train loss: 22.4667 test loss: 14.2577\n",
      "itertion 450 train loss: 6.75931 test loss: 14.2448\n",
      "itertion 460 train loss: 25.5782 test loss: 14.2413\n",
      "itertion 470 train loss: 17.5897 test loss: 14.2269\n",
      "itertion 480 train loss: 22.0697 test loss: 14.2216\n",
      "itertion 490 train loss: 15.2638 test loss: 14.2201\n"
     ]
    }
   ],
   "source": [
    "feed_size = 100\n",
    "n_input = 101 * 101 * 60\n",
    "X = tf.placeholder(tf.int8,[None,n_input])\n",
    "W = tf.Variable(tf.zeros([n_input,1]),tf.float32)\n",
    "b = tf.Variable([0.0],tf.float32)\n",
    "y = tf.add(tf.matmul(tf.to_float(X),W),b)\n",
    "y_ = tf.placeholder(tf.float32)\n",
    "resm = tf.reduce_sum(tf.square(tf.transpose(y) - y_))\n",
    "\n",
    "train_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), feed_size))\n",
    "real_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), 1000))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.00000000000001).minimize(resm)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(500):\n",
    "    indexbegin = feed_size * i % 9000\n",
    "    batch_xs,batch_ys = train_x[indexbegin:indexbegin + feed_size,:],train_y[indexbegin:indexbegin + feed_size]\n",
    "    if i % 10 == 0:\n",
    "        print('itertion %s' % (i),end=' ')\n",
    "        curr_loss = sess.run(train_resm,feed_dict={X:batch_xs,y_:batch_ys})\n",
    "        print (\"train loss: %s\" % (curr_loss),end=' ')\n",
    "        curr_loss = sess.run(real_resm,feed_dict={X:test_x,y_:test_y})\n",
    "        print (\"test loss: %s\" % (curr_loss))\n",
    "    sess.run(train_step,feed_dict={X:batch_xs,y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-60-d39adc9d06ac>:14: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "itertion 0 train loss: 6.82919 test loss: 17.9016\n",
      "itertion 10 train loss: 29.1683 test loss: 17.2526\n",
      "itertion 20 train loss: 19.4203 test loss: 16.6113\n",
      "itertion 30 train loss: 25.0982 test loss: 16.1989\n",
      "itertion 40 train loss: 16.8922 test loss: 15.9486\n",
      "itertion 50 train loss: 12.5061 test loss: 15.6573\n",
      "itertion 60 train loss: 20.6948 test loss: 15.3317\n",
      "itertion 70 train loss: 27.9408 test loss: 15.0655\n",
      "itertion 80 train loss: 23.4719 test loss: 14.9267\n",
      "itertion 90 train loss: 5.81648 test loss: 14.7414\n",
      "itertion 100 train loss: 26.5748 test loss: 14.6321\n",
      "itertion 110 train loss: 17.6394 test loss: 14.4432\n",
      "itertion 120 train loss: 22.4769 test loss: 14.3649\n",
      "itertion 130 train loss: 15.5398 test loss: 14.3329\n",
      "itertion 140 train loss: 12.1298 test loss: 14.2845\n",
      "itertion 150 train loss: 17.5755 test loss: 14.2396\n",
      "itertion 160 train loss: 25.7794 test loss: 14.217\n",
      "itertion 170 train loss: 22.2046 test loss: 14.2118\n",
      "itertion 180 train loss: 7.35595 test loss: 14.2148\n",
      "itertion 190 train loss: 25.1052 test loss: 14.2152\n",
      "itertion 200 train loss: 17.7259 test loss: 14.2545\n",
      "itertion 210 train loss: 21.7579 test loss: 14.2779\n",
      "itertion 220 train loss: 14.9351 test loss: 14.2847\n",
      "itertion 230 train loss: 12.7874 test loss: 14.3123\n",
      "itertion 240 train loss: 16.0284 test loss: 14.3506\n",
      "itertion 250 train loss: 24.5634 test loss: 14.3976\n",
      "itertion 260 train loss: 21.5074 test loss: 14.4176\n",
      "itertion 270 train loss: 8.73886 test loss: 14.4919\n",
      "itertion 280 train loss: 24.2507 test loss: 14.4512\n",
      "itertion 290 train loss: 18.2068 test loss: 14.587\n"
     ]
    }
   ],
   "source": [
    "feed_size = 100\n",
    "n_input = 101 * 101 * 60\n",
    "X = tf.placeholder(tf.int8,[None,n_input])\n",
    "W = tf.Variable(tf.zeros([n_input,1]),tf.float32)\n",
    "b = tf.Variable([0.0],tf.float32)\n",
    "y = tf.add(tf.matmul(tf.to_float(X),W),b)\n",
    "y_ = tf.placeholder(tf.float32)\n",
    "resm = tf.reduce_sum(tf.square(tf.transpose(y) - y_))\n",
    "\n",
    "train_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), feed_size))\n",
    "real_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), 1000))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.00000000000003).minimize(resm)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(300):\n",
    "    indexbegin = feed_size * i % 9000\n",
    "    batch_xs,batch_ys = train_x[indexbegin:indexbegin + feed_size,:],train_y[indexbegin:indexbegin + feed_size]\n",
    "    if i % 10 == 0:\n",
    "        print('itertion %s' % (i),end=' ')\n",
    "        curr_loss = sess.run(train_resm,feed_dict={X:batch_xs,y_:batch_ys})\n",
    "        print (\"train loss: %s\" % (curr_loss),end=' ')\n",
    "        curr_loss = sess.run(real_resm,feed_dict={X:test_x,y_:test_y})\n",
    "        print (\"test loss: %s\" % (curr_loss))\n",
    "    sess.run(train_step,feed_dict={X:batch_xs,y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_x = np.concatenate((train_x,test_x))\n",
    "train_y = train_y + test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_x = np.asarray(train_x,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 612060), 10000)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape,len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del test_x\n",
    "del test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_x,test_y = train_x[9000:],train_y[9000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-67-ea9a3656e368>:14: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "itertion 0 test loss: 17.9016\n",
      "itertion 10 test loss: 17.2526\n",
      "itertion 20 test loss: 16.6113\n",
      "itertion 30 test loss: 16.1989\n",
      "itertion 40 test loss: 15.9486\n",
      "itertion 50 test loss: 15.6573\n",
      "itertion 60 test loss: 15.3317\n",
      "itertion 70 test loss: 15.0655\n",
      "itertion 80 test loss: 14.9267\n",
      "itertion 90 test loss: 14.7414\n",
      "itertion 100 test loss: 14.6425\n",
      "itertion 110 test loss: 14.551\n",
      "itertion 120 test loss: 14.3846\n",
      "itertion 130 test loss: 14.3192\n",
      "itertion 140 test loss: 14.2939\n",
      "itertion 150 test loss: 14.2549\n",
      "itertion 160 test loss: 14.2206\n"
     ]
    }
   ],
   "source": [
    "feed_size = 100\n",
    "n_input = 101 * 101 * 60\n",
    "X = tf.placeholder(tf.int8,[None,n_input])\n",
    "W = tf.Variable(tf.zeros([n_input,1]),tf.float32)\n",
    "b = tf.Variable([0.0],tf.float32)\n",
    "y = tf.add(tf.matmul(tf.to_float(X),W),b)\n",
    "y_ = tf.placeholder(tf.float32)\n",
    "resm = tf.reduce_sum(tf.square(tf.transpose(y) - y_))\n",
    "\n",
    "train_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), feed_size))\n",
    "real_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), 1000))\n",
    "\n",
    "train_step = tf.train.GradientDescentOptimizer(0.00000000000003).minimize(resm)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(170):\n",
    "    indexbegin = feed_size * i % 10000\n",
    "    batch_xs,batch_ys = train_x[indexbegin:indexbegin + feed_size,:],train_y[indexbegin:indexbegin + feed_size]\n",
    "    if i % 10 == 0:\n",
    "        print('itertion %s' % (i),end=' ')\n",
    "        #curr_loss = sess.run(train_resm,feed_dict={X:batch_xs,y_:batch_ys})\n",
    "        #print (\"train loss: %s\" % (curr_loss),end=' ')\n",
    "        curr_loss = sess.run(real_resm,feed_dict={X:test_x,y_:test_y})\n",
    "        print (\"test loss: %s\" % (curr_loss))\n",
    "    sess.run(train_step,feed_dict={X:batch_xs,y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y\n",
    "del test_x\n",
    "del test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1 0.105 0.11 0.115 0.12 0.125 0.13 0.135 0.14 0.145 0.15 0.155 0.16 0.165 0.17 0.175 0.18 0.185 0.19 0.195 0.2 "
     ]
    }
   ],
   "source": [
    "predict_x = []\n",
    "count = 0\n",
    "with open('CIKM2017_testA/testA.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print (float(count) / 10000,end=' '),\n",
    "        linenum,label,datas = line.strip().split(',')\n",
    "        label = float(label)\n",
    "        datas = np.asarray(datas.split(' '),dtype=np.int8)\n",
    "        predict_x.append(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y = sess.run(y,feed_dict={X:np.asarray(predict_x,dtype=np.int8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.89219189],\n",
       "       [ 10.65508747],\n",
       "       [ 12.67513371],\n",
       "       ..., \n",
       "       [  5.09071112],\n",
       "       [  2.88096619],\n",
       "       [  3.77302599]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 1)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8921919"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answer_4_11.csv','w') as whdl:\n",
    "    for value in predict_y:\n",
    "        whdl.write(\"%s\\n\" % (value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1 0.105 0.11 0.115 0.12 0.125 0.13 0.135 0.14 0.145 0.15 0.155 0.16 0.165 0.17 0.175 0.18 0.185 0.19 0.195 0.2 0.205 0.21 0.215 0.22 0.225 0.23 0.235 0.24 0.245 0.25 0.255 0.26 0.265 0.27 0.275 0.28 0.285 0.29 0.295 0.3 0.305 0.31 0.315 0.32 0.325 0.33 0.335 0.34 0.345 0.35 0.355 0.36 0.365 0.37 0.375 0.38 0.385 0.39 0.395 0.4 0.405 0.41 0.415 0.42 0.425 0.43 0.435 0.44 0.445 0.45 0.455 0.46 0.465 0.47 0.475 0.48 0.485 0.49 0.495 0.5 0.505 0.51 0.515 0.52 0.525 0.53 0.535 0.54 0.545 0.55 0.555 0.56 0.565 0.57 0.575 0.58 0.585 0.59 0.595 0.6 0.605 0.61 0.615 0.62 0.625 0.63 0.635 0.64 0.645 0.65 0.655 0.66 0.665 0.67 0.675 0.68 0.685 0.69 0.695 0.7 0.705 0.71 0.715 0.72 0.725 0.73 0.735 0.74 0.745 0.75 0.755 0.76 0.765 0.77 0.775 0.78 0.785 0.79 0.795 0.8 0.805 0.81 0.815 0.82 0.825 0.83 0.835 0.84 0.845 0.85 0.855 0.86 0.865 0.87 0.875 0.88 0.885 0.89 0.895 0.9 0.905 0.91 0.915 0.92 0.925 0.93 0.935 0.94 0.945 0.95 0.955 0.96 0.965 0.97 0.975 0.98 0.985 0.99 0.995 1.0 "
     ]
    }
   ],
   "source": [
    "train_x = []\n",
    "train_y = []\n",
    "count = 0\n",
    "with open('CIKM2017_train/train.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print (float(count) / 10000,end=' '),\n",
    "        linenum,label,datas = line.strip().split(',')\n",
    "        label = float(label)\n",
    "        datas = np.asarray(datas.split(' '),dtype=np.int8)\n",
    "        train_x.append(datas)\n",
    "        train_y.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "train_x,train_y = pickle.load(open('data.pkl','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10000)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "train_x,test_x,train_y,test_y = train_test_split(train_x,train_y,test_size=0.2,random_state=1123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# do not use this now, will cause future shuffle slow\n",
    "#train_x = np.asarray(train_x,dtype=np.int8)\n",
    "test_x = np.asarray(test_x,dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, 8000, (2000, 612060), 2000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),len(train_y),test_x.shape,len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "train_x,train_y = shuffle(train_x,train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump((train_x,train_y),open('data.pkl','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getsizeof(train_x[0]) * 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-29-4f4b7943c99e>:16: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "itertion 0 train loss: 21.5276 test loss: 22.2155\n",
      "itertion 40 train loss: 16.4931 test loss: 17.1664\n",
      "itertion 80 train loss: 15.9607 test loss: 16.5217\n",
      "itertion 120 train loss: 15.0604 test loss: 16.0855\n",
      "itertion 160 train loss: 15.2414 test loss: 16.0495\n",
      "itertion 200 train loss: 14.3251 test loss: 15.6556\n",
      "itertion 240 train loss: 14.6419 test loss: 15.7629\n",
      "itertion 280 train loss: 13.7675 test loss: 15.3996\n",
      "itertion 320 train loss: 14.1497 test loss: 15.5732\n",
      "itertion 360 train loss: 13.311 test loss: 15.2301\n",
      "itertion 400 train loss: 13.7222 test loss: 15.4347\n",
      "itertion 440 train loss: 12.919 test loss: 15.1122\n",
      "itertion 480 train loss: 13.3409 test loss: 15.3304\n",
      "itertion 520 train loss: 12.5704 test loss: 15.0273\n",
      "itertion 560 train loss: 12.9942 test loss: 15.2501\n",
      "itertion 600 train loss: 12.2531 test loss: 14.9646\n",
      "itertion 640 train loss: 12.6738 test loss: 15.1869\n",
      "itertion 680 train loss: 11.9593 test loss: 14.9175\n",
      "itertion 720 train loss: 12.3745 test loss: 15.1366\n",
      "itertion 760 train loss: 11.6842 test loss: 14.8817\n",
      "itertion 800 train loss: 12.0926 test loss: 15.0959\n",
      "itertion 840 train loss: 11.4245 test loss: 14.8543\n",
      "itertion 880 train loss: 11.8253 test loss: 15.0627\n",
      "itertion 920 train loss: 11.178 test loss: 14.8334\n",
      "itertion 960 train loss: 11.5707 test loss: 15.0356\n",
      "itertion 1000 train loss: 10.943 test loss: 14.8177\n",
      "itertion 1040 train loss: 11.3273 test loss: 15.0134\n",
      "itertion 1080 train loss: 10.7182 test loss: 14.8062\n",
      "itertion 1120 train loss: 11.0938 test loss: 14.9952\n",
      "itertion 1160 train loss: 10.5026 test loss: 14.7981\n",
      "itertion 1200 train loss: 10.8692 test loss: 14.9805\n",
      "itertion 1240 train loss: 10.2955 test loss: 14.7929\n",
      "itertion 1280 train loss: 10.6528 test loss: 14.9685\n",
      "itertion 1320 train loss: 10.0962 test loss: 14.7902\n",
      "itertion 1360 train loss: 10.4437 test loss: 14.9591\n",
      "itertion 1400 train loss: 9.90409 test loss: 14.7897\n",
      "itertion 1440 train loss: 10.2415 test loss: 14.9517\n",
      "itertion 1480 train loss: 9.71881 test loss: 14.791\n",
      "itertion 1520 train loss: 10.0457 test loss: 14.9461\n",
      "itertion 1560 train loss: 9.53992 test loss: 14.7939\n",
      "itertion 1600 train loss: 9.85577 test loss: 14.9422\n",
      "itertion 1640 train loss: 9.36711 test loss: 14.7984\n",
      "itertion 1680 train loss: 9.6714 test loss: 14.9396\n",
      "itertion 1720 train loss: 9.2001 test loss: 14.8041\n",
      "itertion 1760 train loss: 9.49228 test loss: 14.9383\n",
      "itertion 1800 train loss: 9.03864 test loss: 14.8111\n",
      "itertion 1840 train loss: 9.31812 test loss: 14.9381\n",
      "itertion 1880 train loss: 8.88252 test loss: 14.8192\n",
      "itertion 1920 train loss: 9.14869 test loss: 14.9388\n",
      "itertion 1960 train loss: 8.73157 test loss: 14.8283\n",
      "itertion 2000 train loss: 8.98378 test loss: 14.9404\n",
      "itertion 2040 train loss: 8.58565 test loss: 14.8383\n",
      "itertion 2080 train loss: 8.8232 test loss: 14.9428\n",
      "itertion 2120 train loss: 8.44462 test loss: 14.8493\n",
      "itertion 2160 train loss: 8.66679 test loss: 14.9459\n",
      "itertion 2200 train loss: 8.30838 test loss: 14.861\n",
      "itertion 2240 train loss: 8.51441 test loss: 14.9496\n",
      "itertion 2280 train loss: 8.17683 test loss: 14.8736\n",
      "itertion 2320 train loss: 8.36592 test loss: 14.9539\n",
      "itertion 2360 train loss: 8.04993 test loss: 14.887\n",
      "itertion 2400 train loss: 8.22123 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-4f4b7943c99e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_resm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval_train_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_y\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mval_train_size\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"train loss: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mcurr_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_resm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtest_y\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"test loss: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mcurr_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO random shuffle after each epoch\n",
    "# modify negative values to zero\n",
    "feed_size = 100\n",
    "n_input = 101 * 101 * 60\n",
    "X = tf.placeholder(tf.int8,[None,n_input])\n",
    "W = tf.Variable(tf.zeros([n_input,1]),tf.float32)\n",
    "b = tf.Variable([0.0],tf.float32)\n",
    "y = tf.add(tf.matmul(tf.to_float(X),W),b)\n",
    "y_ = tf.placeholder(tf.float32)\n",
    "resm = tf.reduce_sum(tf.square(tf.transpose(y) - y_))\n",
    "\n",
    "val_train_size = 1000\n",
    "\n",
    "train_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), val_train_size))\n",
    "real_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), len(test_y)))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0000003).minimize(resm)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(4000):\n",
    "    indexbegin = feed_size * i % len(train_y)\n",
    "    batch_xs,batch_ys = train_x[indexbegin:indexbegin + feed_size,:],train_y[indexbegin:indexbegin + feed_size]\n",
    "    if i % 40 == 0:\n",
    "        print('itertion %s' % (i),end=' ')\n",
    "        curr_loss = sess.run(train_resm,feed_dict={X:train_x[:val_train_size],y_:train_y[:val_train_size]})\n",
    "        print (\"train loss: %s\" % (curr_loss),end=' ')\n",
    "        curr_loss = sess.run(real_resm,feed_dict={X:test_x,y_:test_y})\n",
    "        print (\"test loss: %s\" % (curr_loss))\n",
    "    sess.run(train_step,feed_dict={X:batch_xs,y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = np.asarray([-2,2],dtype=np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 2], dtype=int8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[sample < 0] = 0\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# process negative value\n",
    "for each_ele in train_x:\n",
    "    each_ele[each_ele < 0] = 0\n",
    "test_x[test_x < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8000, (2000, 612060))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_x),test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-23-02f22993bfac>:22: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "shuffleing ...\n",
      "itertion 0 train loss: 22.0375 test loss: 22.2155  check weights_dic[0]\n",
      "itertion 40 train loss: 16.8781 test loss: 17.1524  check weights_dic[40]\n",
      "shuffleing ...\n",
      "itertion 80 train loss: 16.1381 test loss: 16.6459  check weights_dic[80]\n",
      "itertion 120 train loss: 15.7058 test loss: 16.5028  check weights_dic[120]\n",
      "shuffleing ...\n",
      "itertion 160 train loss: 16.2856 test loss: 16.1613  check weights_dic[160]\n",
      "itertion 200 train loss: 15.7893 test loss: 16.0358  check weights_dic[200]\n",
      "shuffleing ...\n",
      "itertion 240 train loss: 15.6402 test loss: 15.806  check weights_dic[240]\n",
      "itertion 280 train loss: 15.3826 test loss: 15.7308  check weights_dic[280]\n",
      "shuffleing ...\n",
      "itertion 320 train loss: 15.4396 test loss: 15.8067  check weights_dic[320]\n",
      "itertion 360 train loss: 14.7605 test loss: 15.5207  check weights_dic[360]\n",
      "shuffleing ...\n",
      "itertion 400 train loss: 14.5223 test loss: 15.5137  check weights_dic[400]\n",
      "itertion 440 train loss: 14.2774 test loss: 15.4396  check weights_dic[440]\n",
      "shuffleing ...\n",
      "itertion 480 train loss: 14.5786 test loss: 15.287  check weights_dic[480]\n",
      "itertion 520 train loss: 14.3966 test loss: 15.3034  check weights_dic[520]\n",
      "shuffleing ...\n",
      "itertion 560 train loss: 14.6607 test loss: 15.7552  check weights_dic[560]\n",
      "itertion 600 train loss: 14.0146 test loss: 15.2804  check weights_dic[600]\n",
      "shuffleing ...\n",
      "itertion 640 train loss: 14.4307 test loss: 15.2988  check weights_dic[640]\n",
      "itertion 680 train loss: 13.9781 test loss: 15.1317  check weights_dic[680]\n",
      "shuffleing ...\n",
      "itertion 720 train loss: 14.1591 test loss: 15.3416  check weights_dic[720]\n",
      "itertion 760 train loss: 13.7293 test loss: 15.0453  check weights_dic[760]\n",
      "shuffleing ...\n",
      "itertion 800 train loss: 14.1112 test loss: 14.9856  check weights_dic[800]\n",
      "itertion 840 train loss: 14.1339 test loss: 15.2921  check weights_dic[840]\n",
      "shuffleing ...\n",
      "itertion 880 train loss: 13.6261 test loss: 14.9998  check weights_dic[880]\n",
      "itertion 920 train loss: 13.5797 test loss: 15.1538  check weights_dic[920]\n",
      "shuffleing ...\n",
      "itertion 960 train loss: 14.0936 test loss: 15.0254  check weights_dic[960]\n",
      "itertion 1000 train loss: 13.913 test loss: 15.0824  check weights_dic[1000]\n",
      "shuffleing ...\n",
      "itertion 1040 train loss: 14.2585 test loss: 14.8904  check weights_dic[1040]\n",
      "itertion 1080 train loss: 14.1387 test loss: 14.9358  check weights_dic[1080]\n",
      "shuffleing ...\n",
      "itertion 1120 train loss: 12.8998 test loss: 14.8772  check weights_dic[1120]\n",
      "itertion 1160 train loss: 12.7959 test loss: 14.9515  check weights_dic[1160]\n",
      "shuffleing ...\n",
      "itertion 1200 train loss: 12.6151 test loss: 14.9696  check weights_dic[1200]\n",
      "itertion 1240 train loss: 12.3932 test loss: 14.8374  check weights_dic[1240]\n",
      "shuffleing ...\n",
      "itertion 1280 train loss: 12.6346 test loss: 15.0479  check weights_dic[1280]\n",
      "itertion 1320 train loss: 12.4121 test loss: 14.8796  check weights_dic[1320]\n",
      "shuffleing ...\n",
      "itertion 1360 train loss: 12.6411 test loss: 14.971  check weights_dic[1360]\n",
      "itertion 1400 train loss: 12.3398 test loss: 14.8696  check weights_dic[1400]\n",
      "shuffleing ...\n",
      "itertion 1440 train loss: 13.0434 test loss: 15.0081  check weights_dic[1440]\n",
      "itertion 1480 train loss: 12.5158 test loss: 14.8022  check weights_dic[1480]\n",
      "shuffleing ...\n",
      "itertion 1520 train loss: 12.4984 test loss: 14.7672  check weights_dic[1520]\n",
      "itertion 1560 train loss: 12.326 test loss: 14.7766  check weights_dic[1560]\n",
      "shuffleing ...\n",
      "itertion 1600 train loss: 12.2818 test loss: 14.8269  check weights_dic[1600]\n",
      "itertion 1640 train loss: 12.1084 test loss: 14.8431  check weights_dic[1640]\n",
      "shuffleing ...\n",
      "itertion 1680 train loss: 11.8825 test loss: 14.7135  check weights_dic[1680]\n",
      "itertion 1720 train loss: 11.7509 test loss: 14.7254  check weights_dic[1720]\n",
      "shuffleing ...\n",
      "itertion 1760 train loss: 12.1266 test loss: 14.7183  check weights_dic[1760]\n",
      "itertion 1800 train loss: 11.9448 test loss: 14.7217  check weights_dic[1800]\n",
      "shuffleing ...\n",
      "itertion 1840 train loss: 12.4945 test loss: 14.804  check weights_dic[1840]\n",
      "itertion 1880 train loss: 12.2738 test loss: 14.7694  check weights_dic[1880]\n",
      "shuffleing ...\n",
      "itertion 1920 train loss: 12.3556 test loss: 14.7033  check weights_dic[1920]\n",
      "itertion 1960 train loss: 12.2535 test loss: 14.7799  check weights_dic[1960]\n",
      "shuffleing ...\n",
      "itertion 2000 train loss: 11.4631 test loss: 14.6751  check weights_dic[2000]\n",
      "itertion 2040 train loss: 11.3869 test loss: 14.7467  check weights_dic[2040]\n",
      "shuffleing ...\n",
      "itertion 2080 train loss: 12.0764 test loss: 15.1383  check weights_dic[2080]\n",
      "itertion 2120 train loss: 11.3505 test loss: 14.7201  check weights_dic[2120]\n",
      "shuffleing ...\n",
      "itertion 2160 train loss: 11.3426 test loss: 14.705  check weights_dic[2160]\n",
      "itertion 2200 train loss: 11.1912 test loss: 14.7282  check weights_dic[2200]\n",
      "shuffleing ...\n",
      "itertion 2240 train loss: 11.6297 test loss: 14.6828  check weights_dic[2240]\n",
      "itertion 2280 train loss: 11.5443 test loss: 14.687  check weights_dic[2280]\n",
      "shuffleing ...\n",
      "itertion 2320 train loss: 11.705 test loss: 14.6842  check weights_dic[2320]\n",
      "itertion 2360 train loss: 11.5135 test loss: 14.6327  check weights_dic[2360]\n",
      "shuffleing ...\n",
      "itertion 2400 train loss: 10.9166 test loss: 14.6576  check weights_dic[2400]\n",
      "itertion 2440 train loss: 10.9211 test loss: 14.7377  check weights_dic[2440]\n",
      "shuffleing ...\n",
      "itertion 2480 train loss: 11.03 test loss: 14.651  check weights_dic[2480]\n",
      "itertion 2520 train loss: 10.9424 test loss: 14.6986  check weights_dic[2520]\n",
      "shuffleing ...\n",
      "itertion 2560 train loss: 10.9659 test loss: 14.6716  check weights_dic[2560]\n",
      "itertion 2600 train loss: 10.8196 test loss: 14.661  check weights_dic[2600]\n",
      "shuffleing ...\n",
      "itertion 2640 train loss: 11.4532 test loss: 14.7179  check weights_dic[2640]\n",
      "itertion 2680 train loss: 11.6297 test loss: 14.9626  check weights_dic[2680]\n",
      "shuffleing ...\n",
      "itertion 2720 train loss: 10.9138 test loss: 14.6831  check weights_dic[2720]\n",
      "itertion 2760 train loss: 10.7496 test loss: 14.6808  check weights_dic[2760]\n",
      "shuffleing ...\n",
      "itertion 2800 train loss: 10.7533 test loss: 14.6364  check weights_dic[2800]\n",
      "itertion 2840 train loss: 10.6153 test loss: 14.6443  check weights_dic[2840]\n",
      "shuffleing ...\n",
      "itertion 2880 train loss: 10.6942 test loss: 14.7481  check weights_dic[2880]\n",
      "itertion 2920 train loss: 10.3589 test loss: 14.6719  check weights_dic[2920]\n",
      "shuffleing ...\n",
      "itertion 2960 train loss: 10.1455 test loss: 14.6789  check weights_dic[2960]\n",
      "itertion 3000 train loss: 10.0031 test loss: 14.6237  check weights_dic[3000]\n",
      "shuffleing ...\n",
      "itertion 3040 train loss: 10.3659 test loss: 14.8209  check weights_dic[3040]\n",
      "itertion 3080 train loss: 9.98151 test loss: 14.6486  check weights_dic[3080]\n",
      "shuffleing ...\n",
      "itertion 3120 train loss: 10.103 test loss: 14.8087  check weights_dic[3120]\n",
      "itertion 3160 train loss: 9.82598 test loss: 14.6458  check weights_dic[3160]\n",
      "shuffleing ...\n",
      "itertion 3200 train loss: 10.3278 test loss: 14.6568  check weights_dic[3200]\n",
      "itertion 3240 train loss: 10.1685 test loss: 14.6292  check weights_dic[3240]\n",
      "shuffleing ...\n",
      "itertion 3280 train loss: 10.1592 test loss: 14.7997  check weights_dic[3280]\n",
      "itertion 3320 train loss: 10.1539 test loss: 14.8509  check weights_dic[3320]\n",
      "shuffleing ...\n",
      "itertion 3360 train loss: 10.2235 test loss: 14.7269  check weights_dic[3360]\n",
      "itertion 3400 train loss: 9.96826 test loss: 14.6454  check weights_dic[3400]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-02f22993bfac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" check weights_dic[%s]\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mweights_dic\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\Program Files\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO use 100% data ,not 80% now\n",
    "\n",
    "weights_dic = {}\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "feed_size = 100\n",
    "n_input = 101 * 101 * 60\n",
    "X = tf.placeholder(tf.int8,[None,n_input])\n",
    "W = tf.Variable(tf.zeros([n_input,1]),tf.float32)\n",
    "b = tf.Variable([0.0],tf.float32)\n",
    "y = tf.add(tf.matmul(tf.to_float(X),W),b)\n",
    "y_ = tf.placeholder(tf.float32)\n",
    "resm = tf.reduce_sum(tf.square(tf.transpose(y) - y_))\n",
    "\n",
    "val_train_size = 1000\n",
    "\n",
    "train_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), val_train_size))\n",
    "real_resm = tf.sqrt(tf.div(tf.reduce_sum(tf.square(tf.transpose(y) - y_)), len(test_y)))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(0.0000003).minimize(resm)\n",
    "init = tf.initialize_all_variables()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for i in range(4000):\n",
    "    indexbegin = feed_size * i % len(train_y)\n",
    "    if i % (len(train_y) / (feed_size)) == 0:\n",
    "        print(\"shuffleing ...\")\n",
    "        train_x,train_y = shuffle(train_x,train_y)\n",
    "    batch_xs,batch_ys = np.asarray(train_x[indexbegin:indexbegin + feed_size],dtype=np.int8),train_y[indexbegin:indexbegin + feed_size]\n",
    "    if i % (len(train_y) / (feed_size * 2)) == 0:\n",
    "        print('itertion %s' % (i),end=' ')\n",
    "        curr_loss = sess.run(train_resm,feed_dict={X:train_x[:val_train_size],y_:train_y[:val_train_size]})\n",
    "        print (\"train loss: %s\" % (curr_loss),end=' ')\n",
    "        curr_loss = sess.run(real_resm,feed_dict={X:test_x,y_:test_y})\n",
    "        print (\"test loss: %s\" % (curr_loss),end=\" \")\n",
    "        print(\" check weights_dic[%s]\" % (i))\n",
    "        weights_dic[i] = (sess.run(W),sess.run(b))\n",
    "    sess.run(train_step,feed_dict={X:batch_xs,y_:batch_ys})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('data/4_11_tf_linear_adam_80_precent_weight.plk','wb') as whdl:\n",
    "    pickle.dump(weights_dic,whdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys([0, 1280, 1560, 2600, 520, 600, 1120, 1040, 1880, 280, 1320, 2080, 560, 40, 240, 2320, 3360, 3120, 3080, 1080, 1600, 480, 1920, 880, 1400, 840, 2440, 80, 3000, 2560, 3160, 1440, 1840, 2400, 1680, 2880, 3400, 2920, 1640, 3200, 720, 1240, 2160, 360, 1760, 3320, 120, 200, 2280, 640, 320, 1160, 1800, 2960, 920, 2680, 2200, 2640, 2720, 2800, 3240, 1200, 2120, 440, 960, 800, 1960, 1480, 160, 2240, 2000, 760, 1360, 2520, 2360, 3040, 3280, 1000, 2840, 1520, 680, 400, 2480, 2040, 1720, 2760])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_dic.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[  2.01886705e-05],\n",
       "        [  2.55818595e-05],\n",
       "        [  1.79752569e-05],\n",
       "        ..., \n",
       "        [  5.49545511e-08],\n",
       "        [ -9.89769433e-06],\n",
       "        [ -8.75896967e-06]], dtype=float32),\n",
       " array([ 0.00021187], dtype=float32))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = weights_dic[1280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del train_x\n",
    "del train_y\n",
    "del test_x\n",
    "del test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 0.01 0.015 0.02 0.025 0.03 0.035 0.04 0.045 0.05 0.055 0.06 0.065 0.07 0.075 0.08 0.085 0.09 0.095 0.1 0.105 0.11 0.115 0.12 0.125 0.13 0.135 0.14 0.145 0.15 0.155 0.16 0.165 0.17 0.175 0.18 0.185 0.19 0.195 0.2 "
     ]
    }
   ],
   "source": [
    "predict_x = []\n",
    "count = 0\n",
    "with open('CIKM2017_testA/testA.txt') as fhdl:\n",
    "    for line in fhdl:\n",
    "        count += 1\n",
    "        if count % 50 == 0:\n",
    "            print (float(count) / 10000,end=' '),\n",
    "        linenum,label,datas = line.strip().split(',')\n",
    "        label = float(label)\n",
    "        datas = np.asarray(datas.split(' '),dtype=np.int8)\n",
    "        predict_x.append(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('onlinedata.plk','wb') as whdl:\n",
    "    pickle.dump(predict_x,whdl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predict_y = sess.run(y,feed_dict={X:np.asarray(predict_x,dtype=np.int8)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7.89219189],\n",
       "       [ 10.65508747],\n",
       "       [ 12.67513371],\n",
       "       ..., \n",
       "       [  5.09071112],\n",
       "       [  2.88096619],\n",
       "       [  3.77302599]], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_w,model_b = weights_dic[1280]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mm = np.dot(np.asarray(predict_x),model_w) + model_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 14.0964489 ,  13.99987507,  14.10657501, ...,  18.26054955,\n",
       "         5.55170965,  13.96566772], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mm[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "160\n",
      "320\n",
      "480\n",
      "640\n",
      "800\n",
      "960\n",
      "1000\n",
      "1040\n",
      "1080\n",
      "1120\n",
      "1160\n",
      "1200\n",
      "1240\n",
      "1280\n",
      "1320\n",
      "1360\n",
      "1400\n",
      "1440\n",
      "1480\n",
      "1520\n",
      "1560\n",
      "1600\n",
      "1640\n",
      "1680\n",
      "1720\n",
      "1760\n",
      "1800\n",
      "1840\n",
      "1880\n",
      "1920\n",
      "1960\n",
      "2000\n",
      "2040\n",
      "2080\n",
      "2120\n",
      "2160\n",
      "2200\n",
      "2240\n",
      "2280\n",
      "2320\n",
      "2360\n",
      "2400\n",
      "2440\n",
      "2480\n",
      "2520\n",
      "2560\n",
      "2600\n",
      "2640\n",
      "2680\n",
      "2720\n"
     ]
    }
   ],
   "source": [
    "for itern in range(0,1000,160):\n",
    "    print(itern)\n",
    "    with open('answers/4_11_tf_linear_adam_80_precent_weight_iter_%s.csv' % itern,'w') as whdl:\n",
    "        model_w,model_b = weights_dic[itern]\n",
    "        predict_y = np.dot(np.asarray(predict_x),model_w) + model_b\n",
    "        for value in predict_y[:,0]:\n",
    "            whdl.write(\"%s\\n\" % (max(value,0)))\n",
    "for itern in range(1000,2760,40):\n",
    "    print (itern)\n",
    "    with open('answers/4_11_tf_linear_adam_80_precent_weight_iter_%s.csv' % itern,'w') as whdl:\n",
    "        model_w,model_b = weights_dic[itern]\n",
    "        predict_y = np.dot(np.asarray(predict_x),model_w) + model_b\n",
    "        for value in predict_y[:,0]:\n",
    "            whdl.write(\"%s\\n\" % (max(value,0)))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.8921919"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_y[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('answer_4_11.csv','w') as whdl:\n",
    "    for value in predict_y:\n",
    "        whdl.write(\"%s\\n\" % (value[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "           "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
